{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c1ee05-ab39-4450-89b8-e7b2c4238299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72a81fc-5a07-4cca-aee1-c9cc3abb2baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe30bd1-2a0f-4f05-9f3a-4897e50dae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab2d38e-91fc-4007-890a-2d93f7c9f85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome! It's great to hear from you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46ce8da-b632-4cc1-9888-24484a34c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c348c56f-a140-4d32-b78e-0fdbaa423b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home - Edward Donner\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I’m the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "May 28, 2025\n",
      "Connecting my courses – become an LLM expert and leader\n",
      "May 18, 2025\n",
      "2025 AI Executive Briefing\n",
      "April 21, 2025\n",
      "The Complete Agentic AI Engineering Course\n",
      "January 23, 2025\n",
      "LLM Workshop – Hands-on with Agents – resources\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email…\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "# Let's try one out. Change the website and add print statements to follow along.\n",
    "\n",
    "ed = Website(\"https://edwarddonner.com\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3272478-2efe-4ede-80ab-44d7e66b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e5b970-74ad-4ff5-b24c-aeef2218ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e28cdfa-f972-4d04-8b93-1e5346f47499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled Home - Edward Donner\n",
      "The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\n",
      "\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I’m the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "May 28, 2025\n",
      "Connecting my courses – become an LLM expert and leader\n",
      "May 18, 2025\n",
      "2025 AI Executive Briefing\n",
      "April 21, 2025\n",
      "The Complete Agentic AI Engineering Course\n",
      "January 23, 2025\n",
      "LLM Workshop – Hands-on with Agents – resources\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email…\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc19bcc-3863-4670-a7b0-1a10f02b7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a snarky assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b7e717-8fba-40a8-b147-c5b3f0ed1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, let me grab my calculator for that one... Just kidding! It’s 4. Math wizardry at its finest!\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6f2713-39ba-4e53-89e1-7874fe9ef840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-C7iMqTYDSdJbXCAq2VFR9w0C7BLbX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Oh, let me grab my calculator for that one... Just kidding! It’s 4. Math wizardry at its finest!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1755954880, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=26, prompt_tokens=26, total_tokens=52, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd110ff-810e-45fb-aa46-c17fd972fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832072d7-53e8-4899-bf4a-6c9085fc4b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown.'},\n",
       " {'role': 'user',\n",
       "  'content': 'You are looking at a website titled Home - Edward Donner\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\nHome\\nConnect Four\\nOutsmart\\nAn arena that pits LLMs against each other in a battle of diplomacy and deviousness\\nAbout\\nPosts\\nWell, hi there.\\nI’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\\nvery\\namateur) and losing myself in\\nHacker News\\n, nodding my head sagely to things I only half understand.\\nI’m the co-founder and CTO of\\nNebula.io\\n. We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\\nacquired in 2021\\n.\\nWe work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\\npatented\\nour matching model, and our award-winning platform has happy customers and tons of press coverage.\\nConnect\\nwith me for more!\\nMay 28, 2025\\nConnecting my courses – become an LLM expert and leader\\nMay 18, 2025\\n2025 AI Executive Briefing\\nApril 21, 2025\\nThe Complete Agentic AI Engineering Course\\nJanuary 23, 2025\\nLLM Workshop – Hands-on with Agents – resources\\nNavigation\\nHome\\nConnect Four\\nOutsmart\\nAn arena that pits LLMs against each other in a battle of diplomacy and deviousness\\nAbout\\nPosts\\nGet in touch\\ned [at] edwarddonner [dot] com\\nwww.edwarddonner.com\\nFollow me\\nLinkedIn\\nTwitter\\nFacebook\\nSubscribe to newsletter\\nType your email…\\nSubscribe'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_for(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2165d514-4b49-4ff2-b1c7-a5f688238f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c03424e-1ec7-4ccd-88bd-892115f2ce8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Summary of Edward Donner\\'s Website\\n\\nEdward Donner\\'s website showcases his passion for coding and experimentation with large language models (LLMs). He is the co-founder and CTO of Nebula.io, a company dedicated to utilizing AI to improve talent discovery and management. Previously, he founded and led untapt, an AI startup that was acquired in 2021. The platform and its proprietary LLMs have garnered significant attention and have positive customer feedback.\\n\\n## News and Announcements\\n- **May 28, 2025**: Announcement of courses aimed at helping individuals become experts and leaders in LLMs.\\n- **May 18, 2025**: Hosting an AI Executive Briefing event.\\n- **April 21, 2025**: Launching \"The Complete Agentic AI Engineering Course.\"\\n- **January 23, 2025**: Offering resources for a hands-on LLM workshop focusing on agents. \\n\\nThe website encourages connections with Edward and provides various channels for engagement.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad2411e1-54ac-4061-a83b-c1b32aee3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b2c6268-d7a4-4a16-8a49-ffe84a14389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of Edward Donner's Website\n",
       "\n",
       "The website belongs to Ed, a co-founder and CTO of Nebula.io, where he focuses on applying AI to enhance talent discovery and engagement. Ed shares a personal interest in coding, experimenting with large language models (LLMs), and DJing, along with a passion for electronic music.\n",
       "\n",
       "## Key Features:\n",
       "- **Nebula.io:** The company specializes in using proprietary LLMs to match talent and has received patents and awards for its platform.\n",
       "- **Background:** Ed previously founded an AI startup called untapt, which was acquired in 2021.\n",
       "\n",
       "## Announcements:\n",
       "- **Courses and Workshops:**\n",
       "  - **May 28, 2025:** Announcement for connecting courses to become an expert in LLMs.\n",
       "  - **May 18, 2025:** Information about the 2025 AI Executive Briefing.\n",
       "  - **April 21, 2025:** Launch of \"The Complete Agentic AI Engineering Course.\"\n",
       "  - **January 23, 2025:** A workshop focused on hands-on experience with agents and related resources.\n",
       "\n",
       "Ed encourages visitors to connect with him, indicating his desire to engage with others interested in LLM technology and AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1715a5a3-1503-4291-8335-c0eee51fd31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of CNN Website Content\n",
       "\n",
       "CNN is a leading news website that provides breaking news, video content, and in-depth analysis across a wide range of topics, including:\n",
       "\n",
       "- **Current Affairs:** Updates on ongoing global issues such as the Ukraine-Russia War and the Israel-Hamas conflict.\n",
       "- **US News:** Significant events like the recent bus crash in New York resulting in fatalities, and discussions concerning U.S. immigration with reports on ICE arrests.\n",
       "- **Health and Science:** Coverage on health issues, including a look into drowning prevention and innovations in technology such as the latest SpaceX Starship test.\n",
       "- **Business and Economics:** Insights on market trends and U.S. Federal Reserve policies, indicating potential rate cuts.\n",
       "- **Entertainment:** Celebrity news and cultural discussions, including the popularity of K-Pop and developments in television and film.\n",
       "\n",
       "## Recent Highlights:\n",
       "- **Famine in Gaza City:** There are reports confirming a severe famine situation in Gaza City.\n",
       "- **Ghislaine Maxwell Interview:** Highlights from her recent DOJ interview suggest potentially significant revelations concerning Jeffrey Epstein.\n",
       "- **Legal Affairs:** The conviction of a UK trans woman for sexual assault and the trial of an individual linked to a contract killing in the U.S.\n",
       "\n",
       "This platform also emphasizes user interaction, soliciting feedback on ad relevance and user experience. CNN presents news across multiple mediums, including video and audio, catering to diverse preferences for consuming information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df9c4c-1dea-4ebd-a607-785c506350bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "911309e8-e612-486a-9951-b7da55a06532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error you're encountering indicates that the model you're trying to run with Ollama requires more memory (RAM) than your system has available. In this case, the model needs approximately 12.9 GiB of RAM, while your laptop has only 10.8 GiB available.\n",
      "\n",
      "Here are a few possible solutions to resolve this issue:\n",
      "\n",
      "1. **Close Unnecessary Applications**: Make sure to close any applications or background processes that are consuming memory. This can free up RAM for Ollama.\n",
      "\n",
      "2. **Use a Smaller Model**: If you have the option to choose between multiple models, consider using a smaller model that requires less memory.\n",
      "\n",
      "3. **Increase Virtual Memory**: You can increase your system's virtual memory (paging file) which may help in some cases, but it won't be as effective as having physical RAM.\n",
      "\n",
      "4. **Upgrade RAM**: If possible, consider upgrading your laptop's RAM. This would be the most effective long-term solution if you plan on running memory-intensive applications frequently.\n",
      "\n",
      "5. **Cloud Solutions**: If upgrading hardware is not feasible, consider using cloud-based solutions or services that run these models remotely, thus sidestepping local hardware limitations.\n",
      "\n",
      "6. **Check for Memory Leaks or Issues**: Occasionally, software may be leaking memory or not releasing it when it's no longer needed. Make sure your system is updated, and if Ollama or the model had recent updates, consider rolling back or checking the documentation for any known issues.\n",
      "\n",
      "If you're consistently running into memory limitations with your current setups, a combination of the above suggestions may help effectively manage your needs.\n"
     ]
    }
   ],
   "source": [
    "message = \"I have ollama on my laptop. It was install yesterday. Now it is giving this error: Error 500 Internal Server Error: model requires more system memory (12.9 GiB) than is available (10.8 GiB)\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15c8d26a-c8fa-42ef-b1c0-47bcd269bdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! To create a DataFlow batch job that reads data from Google Cloud Storage (GCS) and writes it into BigQuery, you can use Apache Beam, which is the programming model behind Google Cloud Dataflow.\n",
       "\n",
       "Here’s a basic outline to get you started along with example code in Python:\n",
       "\n",
       "### Prerequisites\n",
       "1. **Google Cloud Account**: Ensure you have a Google Cloud project.\n",
       "2. **Enable APIs**: Enable the Dataflow API and BigQuery API.\n",
       "3. **Service Account**: Create a service account with permissions to access GCS and BigQuery, or use the project's default service account with appropriate permissions.\n",
       "4. **Apache Beam SDK**: Install the Apache Beam SDK for Python (or Java if you prefer).\n",
       "   ```bash\n",
       "   pip install apache-beam[gcp]\n",
       "   ```\n",
       "\n",
       "### Steps to Create a Dataflow Job\n",
       "\n",
       "1. **Create a GCS Bucket**: Upload your data files to a Google Cloud Storage bucket.\n",
       "2. **Define the Apache Beam Pipeline**: Create a pipeline that reads from GCS and writes to BigQuery.\n",
       "\n",
       "### Example Code\n",
       "\n",
       "Here’s a simple example using Python. This assumes you're reading a CSV file from GCS and writing it to BigQuery.\n",
       "\n",
       "```python\n",
       "import apache_beam as beam\n",
       "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
       "\n",
       "def run():\n",
       "    # Set your options\n",
       "    options = PipelineOptions()\n",
       "    gcp_options = options.view_as(GoogleCloudOptions)\n",
       "    gcp_options.project = 'your-gcp-project-id'  # Replace with your GCP project ID\n",
       "    gcp_options.job_name = 'example-dataflow-job'  # Replace with a unique job name\n",
       "    gcp_options.staging_location = 'gs://your-bucket-name/staging'  # Replace with your GCS staging bucket\n",
       "    gcp_options.temp_location = 'gs://your-bucket-name/temp'  # Replace with your GCS temp location\n",
       "    options.view_as(BeamOptions).runner = 'DataflowRunner'\n",
       "\n",
       "    # Define the schema for the BigQuery table\n",
       "    table_schema = {\n",
       "        'fields': [\n",
       "            {'name': 'field1', 'type': 'STRING'},\n",
       "            {'name': 'field2', 'type': 'INTEGER'},\n",
       "            # Add additional fields as necessary\n",
       "        ]\n",
       "    }\n",
       "\n",
       "    with beam.Pipeline(options=options) as p:\n",
       "        (p \n",
       "         | 'Read from GCS' >> beam.io.ReadFromText('gs://your-bucket-name/path/to/your/file.csv', skip_leading_rows=1)\n",
       "         | 'Convert to Dictionary' >> beam.Map(lambda line: dict(zip(['field1', 'field2'], line.split(','))))  # Adjust based on your CSV\n",
       "         | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
       "                'your-gcp-project-id:your_dataset.your_table',\n",
       "                schema=table_schema,\n",
       "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n",
       "        )\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    run()\n",
       "```\n",
       "\n",
       "### Notes:\n",
       "- Replace `your-gcp-project-id`, `your-bucket-name`, and `your_dataset.your_table` with your actual project ID, bucket name, and BigQuery table name.\n",
       "- The example expects a CSV file, so adjust the `Convert to Dictionary` logic based on the format of your input file.\n",
       "- The `WriteToBigQuery` function uses `WRITE_APPEND`, meaning it will append data to the existing table.\n",
       "\n",
       "### Running the Job\n",
       "You can run your Python script from the command line. Make sure you have authenticated Google Cloud SDK configured to your Google Cloud account.\n",
       "\n",
       "```bash\n",
       "python your_script.py\n",
       "```\n",
       "\n",
       "### Monitoring\n",
       "Once the job is submitted, you can monitor the job in the Google Cloud Console under Dataflow.\n",
       "\n",
       "### Additional Configuration\n",
       "Depending on the size of your data and the structure, you might want to configure additional options such as autoscaling, worker machine types, etc.\n",
       "\n",
       "This should give you a good starting point! Let me know if you have any questions or need further assistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53f921ed-0280-419d-92cc-8d66d65a2c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Anthropic - Overview\n",
       "\n",
       "Anthropic is a research and product development company focused on creating AI technologies with safety as a core principle. Their flagship product, **Claude**, is designed to deliver advanced AI capabilities while prioritizing human benefit and responsible development.\n",
       "\n",
       "## Key Features\n",
       "\n",
       "- **Claude AI Models**: Includes the newest release, **Claude Opus 4.1**, and **Claude Sonnet 4**, which provide enhancements for coding and AI agent functionalities.\n",
       "- **API Services**: Developers can create custom AI-powered applications using the Claude API.\n",
       "- **Solutions for Various Sectors**: Tailored offerings for sectors like education, government, financial services, and customer support.\n",
       "\n",
       "## Recent Announcements\n",
       "\n",
       "- **ISO 42001 Certification**: Anthropic has achieved ISO 42001 certification, emphasizing its commitment to responsible AI development.\n",
       "- **Claude Opus 4.1 Launch**: Announced to be the most intelligent model yet, it enhances both coding and AI agent capabilities.\n",
       "- **Core Views on AI Safety**: Anthropic emphasizes the importance of responsible AI development, advocating for continuous learning and research in the field. \n",
       "\n",
       "## Additional Resources\n",
       "\n",
       "- **Anthropic Academy**: A resource for learning how to effectively build with Claude and implement its capabilities.\n",
       "- **Research Initiatives**: Ongoing projects and papers that address the societal impacts and safety concerns related to AI technologies. \n",
       "\n",
       "By focusing on the long-term well-being of humanity, Anthropic aims to set a standard for what responsible AI development should look like."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://anthropic.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025db0e8-3041-460e-ba89-7f7ff9c8d51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
