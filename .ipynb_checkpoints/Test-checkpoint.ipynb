{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa7ea38-bf6d-4d3d-af96-29f354693ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr # oh yeah!\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d272696-c185-4db2-a2ea-39ecbeda1305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyAt\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968e62bf-7a4d-4939-85ab-9a7148adc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c410449c-91d8-4e08-a7bf-7b45dfeb609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you please create a test Dataset for me for Service now Incidents with few open and closed incidents and count ot be 10000 records\"}\n",
    "  ]\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25df0cb-b1a0-47eb-b0d8-f9bf54f7a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    csv_string =  completion.choices[0].message.content\n",
    "    print (csv_string)\n",
    "    return csv_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29dfddb8-7fa6-45c4-b200-284a0d1e79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Since I cannot create actual files or datasets directly within this text interface, I'll provide you with a sample script in Python that you can run in your local environment to generate a test dataset for ServiceNow incidents. The dataset will include open and closed incidents with a total of 10,000 records.\n",
      "\n",
      "Here's an example of how you can create such a dataset:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import random\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Constants\n",
      "NUM_RECORDS = 10000\n",
      "\n",
      "# Sample data\n",
      "priorities = ['1 - Critical', '2 - High', '3 - Moderate', '4 - Low']\n",
      "states = ['Open', 'Closed']\n",
      "categories = ['Hardware', 'Software', 'Network', 'Database', 'Service Request']\n",
      "\n",
      "# Generate data\n",
      "data = []\n",
      "for i in range(NUM_RECORDS):\n",
      "    incident_number = f\"INC{100000 + i}\"  # Incident numbers starting from INC100000\n",
      "    priority = random.choice(priorities)\n",
      "    state = random.choice(states)\n",
      "    category = random.choice(categories)\n",
      "    short_description = f\"Sample incident description {i+1}\"\n",
      "    created_date = datetime.now() - timedelta(days=random.randint(0, 30))  # Created in the last 30 days\n",
      "    resolved_date = None\n",
      "    \n",
      "    if state == 'Closed':\n",
      "        resolved_date = created_date + timedelta(days=random.randint(1, 7))  # Resolved within a week of creation\n",
      "    else:\n",
      "        resolved_date = None  # Open incidents won't have a resolved date\n",
      "\n",
      "    data.append({\n",
      "        'Incident Number': incident_number,\n",
      "        'Priority': priority,\n",
      "        'State': state,\n",
      "        'Category': category,\n",
      "        'Short Description': short_description,\n",
      "        'Created Date': created_date.strftime('%Y-%m-%d %H:%M:%S'), \n",
      "        'Resolved Date': resolved_date.strftime('%Y-%m-%d %H:%M:%S') if resolved_date else None\n",
      "    })\n",
      "\n",
      "# Create DataFrame\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Save to CSV\n",
      "df.to_csv('servicenow_incidents.csv', index=False)\n",
      "\n",
      "print(\"Dataset created with 10,000 records in 'servicenow_incidents.csv'\")\n",
      "```\n",
      "\n",
      "### Instructions to Run the Code:\n",
      "1. Make sure you have Python installed on your local machine.\n",
      "2. Install the Pandas library if you haven't already, by running `pip install pandas`.\n",
      "3. Copy the code above and save it to a Python file, e.g., `generate_servicenow_dataset.py`.\n",
      "4. Run the script using the command `python generate_servicenow_dataset.py`.\n",
      "5. The script will generate a CSV file named `servicenow_incidents.csv` with 10,000 records.\n",
      "\n",
      "You can adjust the sample data, priorities, states, or any other attributes as needed for your specific use case.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 349, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2284, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2062, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\components\\dataframe.py\", line 453, in postprocess\n",
      "    headers = self.get_headers(value) or self.headers\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\gradio\\components\\dataframe.py\", line 336, in get_headers\n",
      "    return list(pd.read_csv(value).columns)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"C:\\Python\\Anaconda\\envs\\llms\\Lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'Error processing CSV: Error tokenizing data. C error: Expected 3 fields in line 14, saw 4\\n\\nRaw CSV data:\\nCertainly! Since I cannot create actual files or datasets directly within this text interface, I\\'ll provide you with a sample script in Python that you can run in your local environment to generate a test dataset for ServiceNow incidents. The dataset will include open and closed incidents with a total of 10,000 records.\\n\\nHere\\'s an example of how you can create such a dataset:\\n\\n```python\\nimport pandas as pd\\nimport random\\nfrom datetime import datetime, timedelta\\n\\n# Constants\\nNUM_RECORDS = 10000\\n\\n# Sample data\\npriorities = [\\'1 - Critical\\', \\'2 - High\\', \\'3 - Moderate\\', \\'4 - Low\\']\\nstates = [\\'Open\\', \\'Closed\\']\\ncategories = [\\'Hardware\\', \\'Software\\', \\'Network\\', \\'Database\\', \\'Service Request\\']\\n\\n# Generate data\\ndata = []\\nfor i in range(NUM_RECORDS):\\n    incident_number = f\"INC{100000 + i}\"  # Incident numbers starting from INC100000\\n    priority = random.choice(priorities)\\n    state = random.choice(states)\\n    category = random.choice(categories)\\n    short_description = f\"Sample incident description {i+1}\"\\n    created_date = datetime.now() - timedelta(days=random.randint(0, 30))  # Created in the last 30 days\\n    resolved_date = None\\n    \\n    if state == \\'Closed\\':\\n        resolved_date = created_date + timedelta(days=random.randint(1, 7))  # Resolved within a week of creation\\n    else:\\n        resolved_date = None  # Open incidents won\\'t have a resolved date\\n\\n    data.append({\\n        \\'Incident Number\\': incident_number,\\n        \\'Priority\\': priority,\\n        \\'State\\': state,\\n        \\'Category\\': category,\\n        \\'Short Description\\': short_description,\\n        \\'Created Date\\': created_date.strftime(\\'%Y-%m-%d %H:%M:%S\\'), \\n        \\'Resolved Date\\': resolved_date.strftime(\\'%Y-%m-%d %H:%M:%S\\') if resolved_date else None\\n    })\\n\\n# Create DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Save to CSV\\ndf.to_csv(\\'servicenow_incidents.csv\\', index=False)\\n\\nprint(\"Dataset created with 10,000 records in \\'servicenow_incidents.csv\\'\")\\n```\\n\\n### Instructions to Run the Code:\\n1. Make sure you have Python installed on your local machine.\\n2. Install the Pandas library if you haven\\'t already, by running `pip install pandas`.\\n3. Copy the code above and save it to a Python file, e.g., `generate_servicenow_dataset.py`.\\n4. Run the script using the command `python generate_servicenow_dataset.py`.\\n5. The script will generate a CSV file named `servicenow_incidents.csv` with 10,000 records.\\n\\nYou can adjust the sample data, priorities, states, or any other attributes as needed for your specific use case.'\n"
     ]
    }
   ],
   "source": [
    "def process_and_display_csv(prompt):\n",
    "        csv_data = message_gpt(prompt)\n",
    "        # Convert the CSV string to a DataFrame for better display or further processing\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(csv_data))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            return f\"Error processing CSV: {e}\\nRaw CSV data:\\n{csv_data}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=process_and_display_csv,\n",
    "    inputs=gr.Textbox(lines=5, label=\"Enter your prompt for CSV generation:\"),\n",
    "    outputs=gr.Dataframe(label=\"Generated CSV Data\"), # Use gr.Dataframe to display tabular data\n",
    "    title=\"GPT CSV Generator\"\n",
    ")\n",
    "\n",
    "iface.launch(share=False) # Set share=True to get a public link if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747bd04-2f17-4fd8-9495-7c09efbbc3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr.Interface(fn=message_gpt, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d7c771b-de0c-41b7-9f7b-7c2cc01db05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import random\n",
    "import tempfile\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a2cae6d-4685-4dc4-80f9-2764e70b3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_fn(message, history):\n",
    "    history = history or []\n",
    "\n",
    "    # special command: create dataset\n",
    "    if \"create csv\" in message.lower():\n",
    "        df = generate_servicenow_data(10000)\n",
    "        csv_path = \"servicenow_dataset.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        history.append((message, (csv_path, \"ðŸ“‚ Download ServiceNow CSV (10k rows)\")))\n",
    "        return history, \"\"  # clear input\n",
    "\n",
    "    # otherwise, ask GPT\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o\", adjust if needed\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful ServiceNow assistant.\"}] +\n",
    "                 [{\"role\": \"user\", \"content\": m[0]} for m in history if isinstance(m[0], str)] +\n",
    "                 [{\"role\": \"assistant\", \"content\": m[1]} for m in history if isinstance(m[1], str)] +\n",
    "                 [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "    history.append((message, reply))\n",
    "    return history, \"\"  # clear input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dd2787e-6453-46a3-8735-5b086e2a30c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9848\\194227148.py:96: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"ServiceNow Chatbot\", height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_servicenow_data(n=10000):\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    priorities = [\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
    "    impacts = [\"Low\", \"Medium\", \"High\"]\n",
    "    urgencies = [\"Low\", \"Medium\", \"High\"]\n",
    "    statuses = [\"Open\", \"In Progress\", \"Resolved\", \"Closed\"]\n",
    "    categories = [\"Network\", \"Software\", \"Hardware\", \"Database\", \"Security\"]\n",
    "    subcategories = {\n",
    "        \"Network\": [\"VPN\", \"LAN\", \"WAN\"],\n",
    "        \"Software\": [\"Email\", \"OS\", \"Application\"],\n",
    "        \"Hardware\": [\"Laptop\", \"Desktop\", \"Printer\"],\n",
    "        \"Database\": [\"Oracle\", \"MySQL\", \"SQL Server\"],\n",
    "        \"Security\": [\"Phishing\", \"Malware\", \"Access\"],\n",
    "    }\n",
    "    resolution_codes = [\"Solved Remotely\", \"Solved via On-Site\", \"Workaround Provided\", \"Not Reproducible\"]\n",
    "\n",
    "   # Base dates\n",
    "    opened_dates = pd.date_range(\"2025-07-01\", periods=n, freq=\"h\")\n",
    "\n",
    "    # Random statuses\n",
    "    status_selected = np.random.choice(statuses, n, p=[0.3, 0.3, 0.2, 0.2])  # 60% still open/in progress\n",
    "\n",
    "    closed_dates = []\n",
    "    resolution_codes_selected = []\n",
    "    resolved_by = []\n",
    "\n",
    "    for i, status in enumerate(status_selected):\n",
    "        if status in [\"Resolved\", \"Closed\"]:\n",
    "            closed_dt = opened_dates[i] + timedelta(hours=random.randint(1, 72))\n",
    "            closed_dates.append(closed_dt)\n",
    "            resolution_codes_selected.append(random.choice(resolution_codes))\n",
    "            resolved_by.append(f\"user{random.randint(401,600)}\")\n",
    "        else:\n",
    "            closed_dates.append(pd.NaT)  # no close date\n",
    "            resolution_codes_selected.append(None)\n",
    "            resolved_by.append(None)\n",
    "\n",
    "    categories_selected = np.random.choice(categories, n)\n",
    "    subcategories_selected = [random.choice(subcategories[cat]) for cat in categories_selected]\n",
    "\n",
    "    data = {\n",
    "        \"Incident_ID\": [f\"INC{i:06d}\" for i in range(1, n+1)],\n",
    "        \"Opened_At\": opened_dates,\n",
    "        \"Closed_At\": closed_dates,\n",
    "        \"Opened_By\": [f\"user{random.randint(1,200)}\" for _ in range(n)],\n",
    "        \"Assigned_To\": [f\"user{random.randint(201,400)}\" for _ in range(n)],\n",
    "        \"Priority\": np.random.choice(priorities, n),\n",
    "        \"Impact\": np.random.choice(impacts, n),\n",
    "        \"Urgency\": np.random.choice(urgencies, n),\n",
    "        \"Category\": categories_selected,\n",
    "        \"Subcategory\": subcategories_selected,\n",
    "        \"Status\": status_selected,\n",
    "        \"Resolution_Code\": resolution_codes_selected,\n",
    "        \"Resolved_By\": resolved_by,\n",
    "        \"Short_Description\": [f\"Issue {i} - {random.choice(categories)} related\" for i in range(1, n+1)],\n",
    "        \"Description\": [f\"Detailed description of incident {i}, auto-generated for testing.\" for i in range(1, n+1)],\n",
    "        \"Updated_At\": [d + timedelta(hours=random.randint(1,100)) for d in opened_dates],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "def chatbot_fn(message, history):\n",
    "    history = history or []\n",
    "\n",
    "    if \"create csv\" in message.lower():\n",
    "        df = generate_servicenow_data(10000)\n",
    "        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        csv_path = \"servicenow_dataset.csv\"\n",
    "        df.to_csv(tmp_file.name, index=False)\n",
    "    \n",
    "        # reply: preview + file download\n",
    "        preview = df.head().to_markdown()\n",
    "        bot_reply = f\"Hereâ€™s a preview of the dataset:\\n\\n{preview}\"\n",
    "\n",
    "        history.append((message, bot_reply))\n",
    "        history.append((\"\", (csv_path, \"ðŸ“‚ Download ServiceNow CSV (10k rows)\")))\n",
    "\n",
    "        return history, \"\"  # clear input\n",
    "\n",
    "\n",
    "    # otherwise, ask GPT\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o\", adjust if needed\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful ServiceNow assistant.\"}] +\n",
    "                 [{\"role\": \"user\", \"content\": m[0]} for m in history if isinstance(m[0], str)] +\n",
    "                 [{\"role\": \"assistant\", \"content\": m[1]} for m in history if isinstance(m[1], str)] +\n",
    "                 [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "    history.append((message, reply))\n",
    "    return history, \"\"  # clear input\n",
    "\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"ServiceNow Chatbot\", height=500)\n",
    "    msg = gr.Textbox(\n",
    "        label=\"Type your message here\",\n",
    "        placeholder=\"Ask me something or type 'create csv'...\",\n",
    "    )\n",
    "\n",
    "    msg.submit(chatbot_fn, [msg, chatbot], [chatbot, msg])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b034ce1-a846-41b0-aa5a-aa32b8d66d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
