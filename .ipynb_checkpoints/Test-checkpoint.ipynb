{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa7ea38-bf6d-4d3d-af96-29f354693ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr # oh yeah!\n",
    "import pandas as pd\n",
    "import io\n",
    "# imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader,  PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d272696-c185-4db2-a2ea-39ecbeda1305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyAt\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968e62bf-7a4d-4939-85ab-9a7148adc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b310360e-a0c0-4f76-87a7-fd72bb3f67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62453731-54e0-4edd-84d8-411b3bf30096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 210 documents\n"
     ]
    }
   ],
   "source": [
    "folders = glob.glob(\"pdf/*.pdf\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "for folder in folders:\n",
    "    doc_type = os.path.splitext(os.path.basename(folder))[0]\n",
    "\n",
    "    # Load all PDFs inside this folder\n",
    "    loader = PyPDFLoader(folder)\n",
    "    folder_docs = loader.load()\n",
    "\n",
    "    # Attach folder name as metadata\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc39d1b1-f71a-4f0d-9439-8ebcbaca6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "#documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a97ab40b-9e72-4e49-961d-126150a1f3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0cad94f-7f62-4a8e-899c-1dc32408db9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 241 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2fba7bb-aba8-4e0d-80f5-b3b6c4c62072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='89\n",
      "IIMA HR Policy Manual 2023\n",
      "4. Financial upgradation under the scheme shall be allowed in the immediate next higher level \n",
      "pay in the hierarchy of revised levels as per the policy of the Institute.\n",
      "5. Financial upgradation would be on non-functional basis (i.e. Group D employee is categorised \n",
      "as Group D only) subject to eligibility and within the Group. \n",
      "6. As such there shall be no additional financial upgradation for the senior employee on the \n",
      "ground that the junior employee in the level has got higher pay under the Scheme. If the \n",
      "senior employee remains on LWP or/and does not enhance the education qualification, then \n",
      "possible that a junior employee may get an advantage compared to a senior employee.\n",
      "7. Following are the existing Level change in the Institute for Group D employees:\n",
      "Sr. Level Type of \n",
      "Promotion Minimum service in the Institute (from DOJ)\n",
      "1 1 -- Entry\n",
      "2 2 SP #1 10 years\n",
      "3 3 SP #2 20 years\n",
      "4 4 SR #1 27 years\n",
      "5 5 SR #2 33/35 years (33 years to be considered, in case \n",
      "employee is retiring in the year immediate after \n",
      "review period/year).\n",
      "5. PROMOTION POLICY FOR EXISTING GROUP D EMPLOYEES TO GROUP C \n",
      "(PROMOTION WITH GROUP CHANGE) AT ONE LEVEL HIGHER IN GROUP C \n",
      "TO THE CURRENT LEVEL IN GROUP D:\n",
      "1. Methodology: Written Test and Interview.\n",
      "2. Eligibility: Group D employee of IIMA,,\n",
      "i. with minimum graduation from a recognised university at the time of review,\n",
      "ii. minimum service of 5 years,\n",
      "iii. with “very good” remarks in the Performance Evaluation Reports during last \n",
      "five years.\n",
      "iv. fulfilling all other financial upgradation/ promotion criteria.\n",
      "3. Syllabus: The syllabus of the written test may be as follows:\n",
      "Sr. Topic Marks\n",
      "1 General English (Written and Spoken – equivalent to SCOPE \n",
      "syllabus defined by Government of Gujarat) – Lower English\n",
      "30\n",
      "3 Computer awareness (MS-Office-Word, Excel, Power Point, Internet, \n",
      "E-Mail) equivalent to CCC of Govt.of Gujarat \n",
      "25\n",
      "4 Numerical ability (similar to Bank’s clerk examination) 15\n",
      "5 Reasoning ability (similar to Bank’s clerk examination) 15\n",
      "6 General awareness (similar to Bank’s clerk examination) 15\n",
      "Total 100' metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.1 (Windows)', 'creationdate': '2023-01-27T09:40:38+05:30', 'moddate': '2023-01-27T09:40:47+05:30', 'trapped': '/False', 'source': 'pdf\\\\HR Policy Manual 2023.pdf', 'total_pages': 208, 'page': 98, 'page_label': '89', 'doc_type': 'HR Policy Manual 2023'}\n"
     ]
    }
   ],
   "source": [
    "print (chunks[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a76a678-a1ef-4a1d-9127-38727d2d921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 333 documents\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "059fe893-c783-4d6f-adf1-27aa3d1f7d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "539d1329-87f2-43e1-bc3a-0425aac70056",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Aditya_Solanki_Cloud' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m documents = result[\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m doc_types = [metadata[\u001b[33m'\u001b[39m\u001b[33mdoc_type\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m metadata \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m colors = \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mblue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgreen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mred\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morange\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproducts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memployees\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontracts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcompany\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc_types\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      5\u001b[39m documents = result[\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m doc_types = [metadata[\u001b[33m'\u001b[39m\u001b[33mdoc_type\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m metadata \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m colors = [[\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgreen\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mred\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33morange\u001b[39m\u001b[33m'\u001b[39m][\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproducts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memployees\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontracts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcompany\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc_types]\n",
      "\u001b[31mValueError\u001b[39m: 'Aditya_Solanki_Cloud' is not in list"
     ]
    }
   ],
   "source": [
    "# Prework\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410449c-91d8-4e08-a7bf-7b45dfeb609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you please create a test Dataset for me for Service now Incidents with few open and closed incidents and count ot be 10000 records\"}\n",
    "  ]\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25df0cb-b1a0-47eb-b0d8-f9bf54f7a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    csv_string =  completion.choices[0].message.content\n",
    "    print (csv_string)\n",
    "    return csv_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfddb8-7fa6-45c4-b200-284a0d1e79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_display_csv(prompt):\n",
    "        csv_data = message_gpt(prompt)\n",
    "        # Convert the CSV string to a DataFrame for better display or further processing\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(csv_data))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            return f\"Error processing CSV: {e}\\nRaw CSV data:\\n{csv_data}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=process_and_display_csv,\n",
    "    inputs=gr.Textbox(lines=5, label=\"Enter your prompt for CSV generation:\"),\n",
    "    outputs=gr.Dataframe(label=\"Generated CSV Data\"), # Use gr.Dataframe to display tabular data\n",
    "    title=\"GPT CSV Generator\"\n",
    ")\n",
    "\n",
    "iface.launch(share=False) # Set share=True to get a public link if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747bd04-2f17-4fd8-9495-7c09efbbc3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr.Interface(fn=message_gpt, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c771b-de0c-41b7-9f7b-7c2cc01db05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import random\n",
    "import tempfile\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2cae6d-4685-4dc4-80f9-2764e70b3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_fn(message, history):\n",
    "    history = history or []\n",
    "\n",
    "    # special command: create dataset\n",
    "    if \"create csv\" in message.lower():\n",
    "        df = generate_servicenow_data(10000)\n",
    "        csv_path = \"servicenow_dataset.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        history.append((message, (csv_path, \"📂 Download ServiceNow CSV (10k rows)\")))\n",
    "        return history, \"\"  # clear input\n",
    "\n",
    "    # otherwise, ask GPT\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o\", adjust if needed\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful ServiceNow assistant.\"}] +\n",
    "                 [{\"role\": \"user\", \"content\": m[0]} for m in history if isinstance(m[0], str)] +\n",
    "                 [{\"role\": \"assistant\", \"content\": m[1]} for m in history if isinstance(m[1], str)] +\n",
    "                 [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "    history.append((message, reply))\n",
    "    return history, \"\"  # clear input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2787e-6453-46a3-8735-5b086e2a30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_servicenow_data(n=10000):\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    priorities = [\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
    "    impacts = [\"Low\", \"Medium\", \"High\"]\n",
    "    urgencies = [\"Low\", \"Medium\", \"High\"]\n",
    "    statuses = [\"Open\", \"In Progress\", \"Resolved\", \"Closed\"]\n",
    "    categories = [\"Network\", \"Software\", \"Hardware\", \"Database\", \"Security\"]\n",
    "    subcategories = {\n",
    "        \"Network\": [\"VPN\", \"LAN\", \"WAN\"],\n",
    "        \"Software\": [\"Email\", \"OS\", \"Application\"],\n",
    "        \"Hardware\": [\"Laptop\", \"Desktop\", \"Printer\"],\n",
    "        \"Database\": [\"Oracle\", \"MySQL\", \"SQL Server\"],\n",
    "        \"Security\": [\"Phishing\", \"Malware\", \"Access\"],\n",
    "    }\n",
    "    resolution_codes = [\"Solved Remotely\", \"Solved via On-Site\", \"Workaround Provided\", \"Not Reproducible\"]\n",
    "\n",
    "   # Base dates\n",
    "    opened_dates = pd.date_range(\"2025-07-01\", periods=n, freq=\"h\")\n",
    "\n",
    "    # Random statuses\n",
    "    status_selected = np.random.choice(statuses, n, p=[0.3, 0.3, 0.2, 0.2])  # 60% still open/in progress\n",
    "\n",
    "    closed_dates = []\n",
    "    resolution_codes_selected = []\n",
    "    resolved_by = []\n",
    "\n",
    "    for i, status in enumerate(status_selected):\n",
    "        if status in [\"Resolved\", \"Closed\"]:\n",
    "            closed_dt = opened_dates[i] + timedelta(hours=random.randint(1, 72))\n",
    "            closed_dates.append(closed_dt)\n",
    "            resolution_codes_selected.append(random.choice(resolution_codes))\n",
    "            resolved_by.append(f\"user{random.randint(401,600)}\")\n",
    "        else:\n",
    "            closed_dates.append(pd.NaT)  # no close date\n",
    "            resolution_codes_selected.append(None)\n",
    "            resolved_by.append(None)\n",
    "\n",
    "    categories_selected = np.random.choice(categories, n)\n",
    "    subcategories_selected = [random.choice(subcategories[cat]) for cat in categories_selected]\n",
    "\n",
    "    data = {\n",
    "        \"Incident_ID\": [f\"INC{i:06d}\" for i in range(1, n+1)],\n",
    "        \"Opened_At\": opened_dates,\n",
    "        \"Closed_At\": closed_dates,\n",
    "        \"Opened_By\": [f\"user{random.randint(1,200)}\" for _ in range(n)],\n",
    "        \"Assigned_To\": [f\"user{random.randint(201,400)}\" for _ in range(n)],\n",
    "        \"Priority\": np.random.choice(priorities, n),\n",
    "        \"Impact\": np.random.choice(impacts, n),\n",
    "        \"Urgency\": np.random.choice(urgencies, n),\n",
    "        \"Category\": categories_selected,\n",
    "        \"Subcategory\": subcategories_selected,\n",
    "        \"Status\": status_selected,\n",
    "        \"Resolution_Code\": resolution_codes_selected,\n",
    "        \"Resolved_By\": resolved_by,\n",
    "        \"Short_Description\": [f\"Issue {i} - {random.choice(categories)} related\" for i in range(1, n+1)],\n",
    "        \"Description\": [f\"Detailed description of incident {i}, auto-generated for testing.\" for i in range(1, n+1)],\n",
    "        \"Updated_At\": [d + timedelta(hours=random.randint(1,100)) for d in opened_dates],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "def chatbot_fn(message, history):\n",
    "    history = history or []\n",
    "\n",
    "    if \"create csv\" in message.lower():\n",
    "        df = generate_servicenow_data(10000)\n",
    "        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        csv_path = \"servicenow_dataset.csv\"\n",
    "        df.to_csv(tmp_file.name, index=False)\n",
    "    \n",
    "        # reply: preview + file download\n",
    "        preview = df.head().to_markdown()\n",
    "        bot_reply = f\"Here’s a preview of the dataset:\\n\\n{preview}\"\n",
    "\n",
    "        history.append((message, bot_reply))\n",
    "        history.append((\"\", (csv_path, \"📂 Download ServiceNow CSV (10k rows)\")))\n",
    "\n",
    "        return history, \"\"  # clear input\n",
    "\n",
    "\n",
    "    # otherwise, ask GPT\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or \"gpt-4o\", adjust if needed\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful ServiceNow assistant.\"}] +\n",
    "                 [{\"role\": \"user\", \"content\": m[0]} for m in history if isinstance(m[0], str)] +\n",
    "                 [{\"role\": \"assistant\", \"content\": m[1]} for m in history if isinstance(m[1], str)] +\n",
    "                 [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "    history.append((message, reply))\n",
    "    return history, \"\"  # clear input\n",
    "\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"ServiceNow Chatbot\", height=500)\n",
    "    msg = gr.Textbox(\n",
    "        label=\"Type your message here\",\n",
    "        placeholder=\"Ask me something or type 'create csv'...\",\n",
    "    )\n",
    "\n",
    "    msg.submit(chatbot_fn, [msg, chatbot], [chatbot, msg])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b034ce1-a846-41b0-aa5a-aa32b8d66d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
